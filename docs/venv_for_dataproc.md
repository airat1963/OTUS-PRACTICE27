# Использование виртуального окружения в Dataproc для запуска PySpark-скриптов

Этот документ описывает процесс использования существующего виртуального окружения Python для запуска PySpark-скриптов в кластере Yandex Dataproc. Виртуальное окружение обеспечивает консистентное исполнение кода независимо от настроек среды на узлах кластера.

## 1. Общий принцип работы

Механизм использования виртуального окружения в Dataproc состоит из следующих шагов:

1. Упаковка существующего виртуального окружения `.venv` в архив `.tar.gz`
2. Загрузка архива в S3-хранилище
3. Указание пути к архиву и настройка PySpark для его использования при запуске задания

## 2. Упаковка виртуального окружения

Проект включает автоматизированный скрипт `scripts/create_venv_archive.sh`, который выполняет следующие задачи:

- Проверяет наличие директории `.venv` в корне проекта
- Создает директорию `venvs` для хранения архива
- Упаковывает окружение в архив, исключая ненужные файлы для уменьшения размера

Для запуска скрипта необходимо:

```bash
chmod +x scripts/create_venv_archive.sh
./scripts/create_venv_archive.sh
```

Результатом работы скрипта будет архив `venvs/fraud_detection_venv.tar.gz`.

## 3. Задачи в Makefile

Для упрощения работы с виртуальными окружениями в проект добавлены следующие задачи Makefile:

### `make create-venv-archive`

Упаковывает существующее виртуальное окружение `.venv` в архив. Результат сохраняется в `venvs/fraud_detection_venv.tar.gz`.

```bash
make create-venv-archive
```

### `make upload-venv-to-bucket`

Загружает созданный архив виртуального окружения в указанный S3-бакет.

```bash
make upload-venv-to-bucket
```

### `make deploy-full`

Комплексная задача, которая выполняет все необходимые шаги для полного развертывания проекта:
- Создание архива виртуального окружения
- Загрузка архива в S3-бакет
- Загрузка исходного кода в S3-бакет
- Загрузка DAG-скриптов в S3-бакет
- Загрузка данных в S3-бакет

```bash
make deploy-full
```

## 4. Настройка DAG для использования виртуального окружения

В DAG `fraud_detection_training.py` внесены изменения для использования загруженного виртуального окружения:

```python
train_model = DataprocCreatePysparkJobOperator(
    task_id="train_fraud_detection_model",
    main_python_file_uri=f"{S3_SRC_BUCKET}fraud_detection_model.py",
    connection_id=YC_SA_CONNECTION.conn_id,
    args=[...],
    properties={
        'spark.submit.deployMode': 'cluster',
        'spark.yarn.dist.archives': f'{S3_VENV_ARCHIVE}#venv',
        'spark.yarn.appMasterEnv.PYSPARK_PYTHON': './venv/bin/python',
        'spark.yarn.appMasterEnv.PYSPARK_DRIVER_PYTHON': './venv/bin/python',
        'spark.executorEnv.PYSPARK_PYTHON': './venv/bin/python'
    }
)
```

Ключевые параметры:
- `spark.yarn.dist.archives` - указывает путь к архиву и символическое имя (`#venv`), которое будет использоваться для доступа к распакованному архиву
- `spark.yarn.appMasterEnv.PYSPARK_PYTHON` - устанавливает путь к Python-интерпретатору для Application Master
- `spark.yarn.appMasterEnv.PYSPARK_DRIVER_PYTHON` - устанавливает путь к Python-интерпретатору для драйвера
- `spark.executorEnv.PYSPARK_PYTHON` - устанавливает путь к Python-интерпретатору для исполнителей

## 5. Как это работает

1. При запуске задания в Dataproc, YARN распаковывает указанный архив в рабочую директорию на каждом узле кластера
2. Символическое имя `venv` становится доступным как поддиректория рабочей директории
3. Параметры `PYSPARK_PYTHON` и `PYSPARK_DRIVER_PYTHON` указывают на Python-интерпретатор внутри распакованного виртуального окружения
4. PySpark использует этот интерпретатор вместо системного Python, тем самым получая доступ ко всем установленным в окружении зависимостям

## 6. Преимущества подхода

1. **Изоляция** - код выполняется в изолированном окружении, не зависящем от системных библиотек
2. **Консистентность** - все узлы используют одинаковую версию Python и библиотек
3. **Переиспользование** - используется существующее окружение разработки без необходимости создавать новое
4. **Управляемость** - можно точно контролировать версии всех зависимостей
5. **Портируемость** - код будет работать одинаково в любом кластере Dataproc
6. **Отсутствие необходимости в настройке кластера** - не требуется устанавливать зависимости на узлы кластера
7. **Автоматизация** - весь процесс создания и загрузки окружения автоматизирован

## 7. Возможные проблемы и их решения

1. **Большой размер архива**
   - Решение: используйте `--exclude` для исключения ненужных файлов при создании архива

2. **Несовместимость бинарных библиотек**
   - Решение: убедитесь, что операционная система, на которой создается виртуальное окружение, совместима с образом Dataproc

3. **Путь к интерпретатору Python**
   - Решение: проверьте правильность пути в параметрах `PYSPARK_PYTHON` и `PYSPARK_DRIVER_PYTHON`

4. **Отсутствие доступа к архиву**
   - Решение: убедитесь, что сервисный аккаунт имеет доступ к бакету S3 с архивом 